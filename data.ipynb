{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\SIVABALAJI S\\Desktop\\PDF_CHAT\\PDF.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\SIVABALAJI S\\\\Desktop\\\\PDF_CHAT\\\\toypdf.pdf', 'page': 0}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\\nNick Barnesh, Ajmal Miani\\naUniversity of Engineering and Technology (UET), Lahore, Pakistan\\nbThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ncUniversity of Technology Sydney (UTS), Sydney, Australia\\ndCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\neKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\nfSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\ngThe University of Melbourne (UoM), Melbourne, Australia\\nhAustralian National University (ANU), Canberra, Australia\\niThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\\non a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\\nconcepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\\nprovide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\\nextensive informative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans, and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over years containing keywords \"Large\\nLanguage Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\\nguage Model +Alignment\".\\nPreprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 2024')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=\"gsttextemb002\",\n",
    "        openai_api_version=\"2023-12-01-preview\",\n",
    "        azure_endpoint = \"AZURE_ENDPOINT\",\n",
    "        api_key= \"OPEN_AI_API_KEY\"\n",
    "        )\n",
    "\n",
    "# Create the FAISS index\n",
    "vectorstore = FAISS.from_documents(pages, embeddings)\n",
    "vectorstore.save_local(\"./db\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
